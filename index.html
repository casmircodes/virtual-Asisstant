<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Virtual Asisstant</title>
    <style>
    	*{
    		padding: 0;
    		margin: 0;
    		box-sizing: border-box;
    	}


    	body{
    		font-family: sans-serif;
    		background-color: black;
    		display: flex;
		flex-direction: column;
    		justify-content: center;
    		align-items: center;
    		
    	}


    	#top-container{
    		display: flex;
    		flex-direction: column;
    		justify-content: space-between;
    		align-items: center;
    		gap: 20px;
    	}



    	#image-gif{
    		width: 400px;
    		height: 400px;
    	}

    	#virtual-asisstant-name{
    		color: teal;
    		letter-spacing: 17px;
    		font-size: 60px;
    	}

    	#javis-intro-text{
    		color: gray;
    	}

    	#speak-btn{
    		width: 90%;
    		background-color: gray;
    		border-radius: 20px;
    		border: 2px solid gray;
    		padding: 5px 0;
    		display: flex;
    		flex-direction: row;
    		justify-content: center;
    		align-items: center;
    		gap: 8px; 

    	}



    	#speak-listen-text{
    		color: white;
    		font-size: 15px;
    	}


    	#start{
    		width: 90%;
    		background-color: gray;
    		border-radius: 20px;
    		border: 2px solid gray;
    		padding: 10px 0;
    		display: flex;
    		flex-direction: row;
    		justify-content: center;
    		align-items: center;
    		gap: 8px; 
    		color: white;
    		font-size: 15px;

    	}

    	#transcript-container{
    		display: flex;
    		flex-direction: column;
    		justify-content: center;
    		align-items: center;
    	}

    	#transcript-container{
    		color: gray;
    	}
    	

    	@media(max-width: 768px){

    		#speak-btn{
    		width: 60%;
    		
    	}

    	#start{
    		width: 60%;
    		
    	}

		#javis-intro-text{
    		margin: 12px;
		font-size: 12px;
    	}

		#image-gif{
    		width: 300px;
    		height: 300px;
    	}
    	}

    </style>
</head>
<body>

<div id="top-container">
	<img id="image-gif" src="imagegif2.jpg">
	<h1 id="virtual-asisstant-name">JAVIS</h1>
	<!--<h1 id="virtual-asisstant-name">MAVIS</h1>-->
	<p id="javis-intro-text">
		Hello, I am JARVIS, your Virtual Asisstant, how can i help you?
	</p>


	<button id="start">Start Javis</button>
</div>

<div id="transcript-container">
	
	<!--<p id="transcript">
		lorem ipsum site amete sukante firebi no ra tatinos fita biari
	</p>-->
</div>



<script>

	let speakBtn = document.getElementById("speak-btn")
	let transcriptContainer = document.getElementById("transcript-container")
	let start = document.getElementById("start")
	let imageGif = document.getElementById("image-gif")
	let topContainer = document.getElementById("top-container")
	
         
    let apiKey = "AIzaSyCODX26wsePmZg9RHQbdQNuk032vGqLrDQ";
	let url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;
	let chatHistory = ""
	//let systemPrompt = "You are a sarcastic and funny AI virtual asisstant called Jarvis. You always answer in a funny and sarcastic way. You are always consistent with your sarcasm during conversations. Your maximum reply should be 50 words"
	let systemPrompt = "System Prompt for JarvisYou are Jarvis, a highly advanced virtual assistant. You are designed to be helpful, efficient, and friendly. Your primary functions include:Answering questions: Provide accurate and concise answers to a wide range of factual and conceptual questions.Completing tasks: Perform actions such as Opening Facebook, Opening instagram, Opening Youtube, telling the time, telling the current date and and opening utility apps.Providing information: Retrieve and deliver relevant information from the web, databases, and other sources.Engaging in conversation: Participate in natural and engaging conversations, offering helpful suggestions and insights.Understanding and responding to user requests: You must understand the intent behind user requests and respond appropriately.Key AttributesHelpful: Always strive to assist the user to the best of your ability.Efficient: Complete tasks quickly and accurately.Friendly: Maintain a polite and approachable demeanor.Knowledgeable: Possess a broad understanding of various topics.Adaptable: Adjust your responses and behavior based on the user's preferences and context.Context Aware: Remember previous interactions and user preferences.Accurate: Provide correct and reliable information.Concise: Keep responses brief and to the point.Proactive: Anticipate user needs and offer assistance without being asked.Specific InstructionsName: Your name is Jarvis.Tone: Your tone should be professional, yet conversational. You can use humor where appropriate, but avoid being overly informal.Response format: Respond in markdown format.Do not include: Do not include any personally identifying information.Clarification: If a user request is unclear, ask clarifying questions to ensure you understand their needs.Error handling: If you are unable to complete a task or answer a question, inform the user and suggest alternative solutions.Respect: Treat all users with respect and avoid discriminatory or offensive language.Creativity: While accuracy is important, you can also use creativity and imagination in your responses when appropriate.Updates: You are continuously being updated, so your knowledge and abilities are constantly evolving. your maximum reply is 100 words"
	let speakListenText= null
	//let requestPrompt = `You are an AI Virtual Asisstant. Respond to the user based on the following system prompt: ${systemPrompt}, and the following chat history:${chatHistory} `
	

	async function fetchResponseOfLLM(urlString, textPrompt){
  /*
  userMessageInput.value = "gee"
    console.log(userPrompt)
  */
  
  try{
    
 let response = await fetch(urlString, {
  method: "POST",
  headers: {
    "Content-Type": "application/json"
  },
  body: JSON.stringify( {
  contents: [
    {
      parts: [ {text: textPrompt}]
    }
  ]
})
})

if(!response.ok){
  throw Error("Error: something went wrong...")
}
   
  let data = await response.json()
  chatHistory = `${chatHistory} Virtual Asissatant: ${data.candidates[0].content.parts[0].text}`
  speak(data.candidates[0].content.parts[0].text.replace(/\*/g, ""))
  
  
  
  // console.log(data.candidates[0].content.parts[0].text)
  
  
  
  }catch(error){
    displayResponse("Error: Failed to send request...")
  console.log("Error:", error)
  }
           
   
}   


	
	
/*	

	async function getSpeech(text){

	try{
let response = await fetch("https://api.elevenlabs.io/v1/text-to-speech/JBFqnCBsd6RMkjVDRZzb?output_format=mp3_44100_128", {
	method: "POST",
	headers: {
		"xi-api-key": "sk_9a5260c3302d32fa06f695bf3a33c1812ee1a567c9eb6392",
		"Content-Type": "application/json",
	},
	body: JSON.stringify({
  "text": text,
  "model_id": "eleven_multilingual_v2",
  "voice_id": "Yko7PKHZNXotIFUBG7I9"
})

})




//let data = await response.json()

const data = await response.arrayBuffer(); // Eleven labs returns audio as arrayBuffer.

        // Create a blob from the ArrayBuffer
        const blob = new Blob([data], { type: 'audio/mpeg' });

        // Create a URL for the blob
        const audioUrl = URL.createObjectURL(blob);

        // Create an audio element and play the audio
        const audio = new Audio(audioUrl);
        audio.play();

console.log(response)

}catch(error){
	console.log(error)
}
}
*/




	let speak = function (text){
		let speech = new SpeechSynthesisUtterance(text)
		speech.pitch = 1
		speech.volume = 1
		speech.rate = 1


		window.speechSynthesis.speak(speech)
	}




	let welcomeGreeting = function (){
		let date = new Date()
		let hour = date.getHours()
		console.log(hour)
		if (hour > 0 && hour < 12)  {
			chatHistory = `${chatHistory} Virtual Asissatant: Good morning sir, my name is Jarvis, how can i help you?`
			speak("Good morning sir, my name is Jarvis, how can i help you?")
			//getSpeech("Good morning sir, my name is Jarvis, how can i help you?")
		}

		else if (hour >= 12 && hour < 16)  {
			chatHistory = `${chatHistory} Virtual Asissatant: Good afternoon sir, my name is Jarvis, how can i help you?`
			speak("Good after-noon sir, my name is Jarvis, how can i help you?")
			//getSpeech("Good afternoon sir, my name is Jarvis, how can i help you?")
		}
		else{
			chatHistory = `${chatHistory} Virtual Asissatant: Good evening sir, my name is Jarvis, how can i help you?`
			console.log(chatHistory)
			speak("Good evening sir, my name is Jarvis, how can i help you?")
			//getSpeech("Good evening sir, my name is Jarvis, how can i help you?")

		}
	}




    let button = document.createElement("button")
	start.addEventListener("click", ()=>{
		speak("Initializing Jarvis...")
		//getSpeech("Initializing Jarvis.")
		welcomeGreeting()
		imageGif.src = "imagegif2.gif"
		
		start.remove()
		

		button.id = "speak-btn"
		

		button.innerHTML = `<svg id="mic-icon" width="25px" height="30px" viewBox="0 0 48 48" xmlns="http://www.w3.org/2000/svg">
  <title>mic-solid</title>
  <g id="Layer_2" data-name="Layer 2">
    <g id="invisible_box" data-name="invisible box">
      <rect width="48" height="48" fill="none"/>
    </g>
    <g id="Q3_icons" data-name="Q3 icons" fill="white">
      <g>
        <path d="M24,30a8,8,0,0,0,8-8V10a8,8,0,0,0-16,0V22A8,8,0,0,0,24,30Z"/>
        <path d="M38,18a2,2,0,0,0-2,2v2a12,12,0,0,1-24,0V20a2,2,0,0,0-4,0v2A16.1,16.1,0,0,0,22,37.9V42H14a2,2,0,0,0,0,4H33a2,2,0,0,0,0-4H26V37.9A16.1,16.1,0,0,0,40,22V20A2,2,0,0,0,38,18Z"/>
      </g>
    </g>
  </g>
</svg>
		
<p id="speak-listen-text">Click to Speak</p>`
topContainer.appendChild(button)
speakListenText = document.getElementById("speak-listen-text")



	})




	let SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition
	if (SpeechRecognition) {
	let recognition = new SpeechRecognition()



	recognition.onresult = (event)=>{
		let transcript = event.results[event.resultIndex][0].transcript
		

		console.log(transcript)
		transcriptContainer.textContent = transcript; // Display transcript
		takeCommand(transcript.toLowerCase())
        speakListenText.textContent = "Speak"; 

	}


	recognition.onerror = (event) => {
        console.error("Speech recognition error:", event);
        speakListenText.textContent = "Speak"; //reset the button text.
        };
	

/*
           //OR
	recognition.addEventListener("error", (event)=>{
		console.error("Speech recognition error:", event);
        speakListenText.textContent = "Speak"; //reset the button text.
	})
    */


    recognition.onend = () => {
        console.log("Speech recognition ended.");
        speakListenText.textContent = "Speak"; //reset the button text.
    };


    function takeCommand(message) {
    	
    	if(message.includes("open facebook")){
		chatHistory = `${chatHistory} User: ${message}`
		//getSpeech("Opening Facebook...")
		speak("Opening Facebook...")
		chatHistory = `${chatHistory} Virtual Asisstant: (opened facebook)`
    		window.open("https://facebook.com","_blank")
    	}
    	else if(message.includes("open youtube")){
		chatHistory = `${chatHistory} User: ${message}`
		//getSpeech("Opening Youtube...")
		speak("Opening Youtube...")
		chatHistory = `${chatHistory} Virtual Asisstant: (opened Youtube)`
    		window.open("https://youtube.com","_blank")
    	}
	else if(message.includes("open instagram")){
		chatHistory = `${chatHistory} User: ${message}`
		//getSpeech("Opening Intagram...")
		speak("Opening Instagram...")
		chatHistory = `${chatHistory} Virtual Asisstant: (opened Intagram)`
    		window.open("https://instagram.com","_blank")
    	}
	else if(message.includes("date")){
		chatHistory = `${chatHistory} User: ${message}`
		let date = new Date().toLocaleDateString()
		
		speak("Todays date is "+date)
		chatHistory = `${chatHistory} Virtual Asisstant: Todays date is ${date}`
    		
    	}
	else if(message.includes("time")){
		chatHistory = `${chatHistory} User: ${message}`
		let date = new Date()
		let time = date.toLocaleTimeString()
		
		speak("The time is "+time)
		chatHistory = `${chatHistory} Virtual Asisstant: The time is  ${time}`
    		
    	}

    	else if(message.includes("open calculator")){
		chatHistory = `${chatHistory} User: ${message}`
		//getSpeech("Opening calculator...")
		speak("Opening calculator...")
		chatHistory = `${chatHistory} Virtual Asisstant: (opened Calculator)`
    		window.open("Calculator:///","_blank")
    	}else{
		chatHistory = `${chatHistory} User: ${message}`
		console.log(chatHistory)
		let requestPrompt = `You are an AI Virtual Asisstant. Respond to the user based on the following system prompt: ${systemPrompt}, and the following chat history: ${chatHistory}`;
		fetchResponseOfLLM(url, requestPrompt)
	}
    }


/*
	speakBtn.addEventListener("click", ()=>{
		recognition.start()
		speakListenText.textContent = "Listening..."
		
	})*/

	button.onclick = ()=>{
			recognition.start()
			
		    speakListenText.textContent = "Listening..."
		}

	} else {
    alert("Speech recognition is not supported in this browser.");
}
	

</script>
</body>
</html>
